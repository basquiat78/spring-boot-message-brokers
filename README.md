# Install Kafka

여기서는 `Zookeeper`가 아닌 `KRaft`기반의 `Kafka`를 사용해 볼 생각이다.

```shell
# 실행
root>brokers>kafka> docker compose up -docker

# 내린다면
root>brokers>kafka> docker compose down -v
```

`docker-compose.yml`에는 `ui`도 설정했다.

실행이후 [kafka ui](http://localhost:8090/)

주키퍼를 사용할 때는 카프카용 `ui`을 따로 썼던 기억이 나는데 아예 같이 제공한다.

군더더기 없고 단순하며 깔끔하다.

# About Kafka

이게 최근에 대용량 트래픽 관련해서 언급되서 최근 기술로 알수 있지만 역사가 꽤 오래되었다.

2010년에 `LinkedIn`에서 내부적으로 사용하기 위해 개발된 것이고 2011년에 아파치 재단에서 등록되면서 알려진 메시지 브로커로 작게 보면 그렇다.

하지만 메시지 브로커 기능을 포함한 `분산 이벤트 스트리밍 플랫폼`이라고 보는게 맞을 것이다.

문득 이런 생각이 들것이다.

```text
RabbitMQ랑 차이가 뭐야???
```

# 파티션 개념

데이터를 토픽으로 나누는 부분까지는 비슷해 보인다.

하지만 카프카는 이 토픽을 여러 파티션에 분할을 한다.

쉽게 말하면 파티션은 로그로 순서대로 디스크에 기록이 되는 방식인데 이걸 카프카에서는 `append-only`로그라고 부른다.

그러면 파티션 단위로 기록을 하냐는 의문이 들 수 있는데 이것은 확장을 위해서이다.

클러스터를 구성한다고 하면 파티션 단위로 클러스터내의 여러 브로커에 분산이 용이하고 이것은 복제도 마찬가이다.

그래서 클러스터를 통한 수평적인 확장이 쉬워진다는 개념이다.

`Consumer`쪽에 특징이 또 있다.

`Consumer Group`이라 해서 같은 그룹내의 소비자들은 파티션 개념으로 파티션을 나눠서 읽어서 처리 가능하게 구현되었다.
이에 대한 장점은 로드밸런싱, 장애 관련 문제를 처리하는데 특화되어 있다고 한다.

여기서 `offset`이라는 개념이 있는데 `Consumer Group`은 이 `offset`을 기준으로 데이터를 읽는다.

이것은 `Consumer`측에서 어디까지 메시지를 읽고 소비했는지 알 수 있게 된다.

# 메시지 보관

`docker-compose.yml`을 보면 `retention` 부분이 눈에 띈다. 

디스크에 기록되는 데이터들은 이 정보를 기준으로 데이터를 보관하고 오래된 데이터를 삭제하는 기능을 가지고 있다.

단순하게 휘발성 메시지가 아니라는 의미인데 `Consumer`측에서 소비한 메시지를 삭제하는 구조가 아니고 로그에 기록하고 기능에 따라 삭제를 한다.

# 확장성

파티션 개념으로 인해서 수평확장에 대한 언급을 했는데 이 파티션을 늘려서 브로커를 추가하면 무중단 운영이 가능하다고 한다.

뭐 이렇게 운영을 해 본 적이 없어서 몸으로 느껴보진 않았지만 많은 사례들이나 동료들의 이야기를 보면 이게 꽤 강력한가 보다.

# 이외의 장점들

그 외에도 `Backpressure`가 가능하다.

뭐 백프레셔는 처리량을 조절하는 부분하는 부분이니 이 부분도 눈에 띄기도 한다.

위에서 `offset`과 일정 기간 로그에 기록한다고 언급했는데 이런 기능으로 `Consumer`측에서 이 `offset`를 세팅해서 기록된 과거 데이터를 다시 읽고 처리가 가능하다.

로그를 다시 분석하거나 할 때 유용하다고 하니 장점이라고 할 수 있을 것이다.

또한 에코시스템이 잘 되어 있다고 한다.

스트림 처리라든가 외부 시스템과의 연동등 장점들이 많은 것이 카프카이다!!

# 자 그래서??

사실 이렇게 보면 카프카를 단순하게 `pub/sub`을 위해서 사용하는 것은 오버스펙이다.

`RabbitMQ`나 그외 메시지 브로커들로도 충분히 커버가 가능하다는 의미이다.

하지만 최소한 카프카를 어떻게 활용해야 하는지는 이 부분에서부터 시작한다고 할 수 있다.

이를 통해서 이후 `Kafka Streams`나 실시간으로 데이터 스트림을 처리하기 위한 분산 처리 프레임워크인 아파치 `Flink`를 사용할 수 있지 않을까?

이벤트 버스로 볼 수 있는 `pub/sub`을 기반으로 다양한 기능을 활용할 수 있는 플랫폼인 만큼 이정도는 알고 이후 아이디어를 얻을 수 있을 것이라 본다.

# 그레이들 세팅

```groovy
implementation("org.springframework.boot:spring-boot-starter-kafka")
// kafka-streams
implementation("org.apache.kafka:kafka-streams")
```

사실 지금같이 그냥 메시지를 주고 받고 무언가를 처리한다면 `kafka-streams`는 불필요하다.

하지만 실시간으로 데이터를 처리해야 한다면 `Streams`방식을 고려해 봐야 한다.

일단 저것도 같이 세팅을 하자.

# 지금까지 한 방식으로 리스너 등록

카프카 역시 래빗엠큐처럼 `@KafkListener`를 제공한다.